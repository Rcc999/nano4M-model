{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf7f397-467f-4f24-a18f-4143eca2c3ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "# COM-304 Foundation Models: Part 2 - nanoMaskGIT\n",
    "\n",
    "#### Goals:\n",
    "\n",
    "The goal of this second part is to familiarize yourself with the following topics:\n",
    "- Bi-directional attention\n",
    "- Encoder-only Transformer (e.g. BERT, MaskGIT, ...) models\n",
    "- Basic masking schemes\n",
    "- Masked modelling on text and images\n",
    "- Masked inference\n",
    "\n",
    "This notebook should give you a solid foundation of working with masked image models.\n",
    "If you want to know more about these topics, please see some of the reading material in the lectures and at the bottom of this notebook, and feel free to ask the TAs.\n",
    "\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "- Your task is to fill in the missing code in the acompagning codebase (highlighted by `???`), run the training loops and evaluate the trained models with this notebook.\n",
    "- Submit the notebook with all cells executed, as well as `nanofm/models/maskgit.py`.\n",
    "- The notebooks are individual homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ba70f-25dd-4bdb-953b-d5e4547569ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 1 Setup\n",
    "\n",
    "Please follow the setup detailed in the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1547acb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 10 18:09:39 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-PCIE-32GB           On  | 00000000:86:00.0 Off |                  Off |\n",
      "| N/A   33C    P0              41W / 250W |    596MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE-32GB           On  | 00000000:D8:00.0 Off |                  Off |\n",
      "| N/A   27C    P0              24W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   3113987      C   ...nt/anaconda3/envs/nanofm/bin/python      592MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3365eba-118f-4f63-96fe-1c9904fbc050",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38061c15-c053-431a-9265-536b778fe5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Switch path to root of project\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "current_folder = globals()['_dh'][0]\n",
    "os.chdir(os.path.dirname(os.path.abspath(current_folder)))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef049477-3435-4543-bef7-acfaea93c2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fc7f551a020>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from nanofm.utils.checkpoint import load_model_from_safetensors\n",
    "from nanofm.data.vision.tokenized_mnist import create_tokenized_mnist_dataloader, detokenize_MNIST\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to False in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32547ef7-1554-4d80-80fa-bf0b3487dca9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 2 Training nanoMaskGIT on MNIST for image generation\n",
    "\n",
    "In this exercise, we will implement a simplified masked generative model, similar to [MaskGIT](https://masked-generative-image-transformer.github.io/). \n",
    "As with our nanoGPT implementation, we will train it on MNIST for image generation, but will also later explore using it for text generation on TinyStories!\n",
    "\n",
    "#### Masked modeling - Training objective\n",
    "\n",
    "In contrast to autoregressive models that are trained to predict the next token given the context so far, masked generative models like MaskGIT are trained to predict any (masked-out) token given any other (non-masked) subset of tokens.\n",
    "Consider the following example: \n",
    "\n",
    "**Original Sentence:**  \n",
    "```\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "```\n",
    "\n",
    "**Masked Training Example (cloze):**  \n",
    "```\n",
    "\"The quick [MASK] fox jumps over the [MASK] dog.\"\n",
    "```\n",
    "\n",
    "**Goal:**  \n",
    "The model must predict:\n",
    "- `[MASK]` → \"brown\"\n",
    "- `[MASK]` → \"lazy\"\n",
    "\n",
    "By repeatedly training the model to predict these randomly masked tokens across a large dataset, MaskGIT learns how tokens fit contextually within sequences.\n",
    "\n",
    "\n",
    "#### Masked modeling - Inference\n",
    "\n",
    "\n",
    "By training a model with randomized masking ratios, we are able to use it to progressively \"unmask\" a fully masked initial sequence.\n",
    "At inference, the model starts with all tokens masked and progressively unmasks tokens in multiple steps. Let's show an inference example generating two (`k=2`) tokens at a time.\n",
    "\n",
    "**Initial Masked Sequence:**  \n",
    "```\n",
    "\"[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\"\n",
    "```\n",
    "\n",
    "**Step-by-step Generation (k=2 tokens at a time):**  \n",
    "\n",
    "- **Step 1:**  \n",
    "```\n",
    "\"The [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] dog.\"\n",
    "```\n",
    "\n",
    "- **Step 2:** (unmask next 2 most confident tokens)  \n",
    "```\n",
    "\"The quick [MASK] [MASK] [MASK] [MASK] [MASK] lazy dog.\"\n",
    "```\n",
    "\n",
    "- **Step 3:** (unmask next 2 tokens)  \n",
    "```\n",
    "\"The quick [MASK] fox [MASK] [MASK] [MASK] lazy dog.\"\n",
    "```\n",
    "\n",
    "- **Step 4:** (unmask next 2 tokens)  \n",
    "```\n",
    "\"The quick brown fox jumps [MASK] the lazy dog.\"\n",
    "```\n",
    "\n",
    "- **Step 5 (Final):** (all tokens unmasked)  \n",
    "```\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "```\n",
    "\n",
    "At each inference step, MaskGIT predicts all masked-out tokens simultaneously (in parallel), and, based on the predicted probabilites, selects the `k` (here `k=2`) most likely tokens. \n",
    "After deciding on which tokens to use, we sample a token index from the predicted probability distribution for each token, and add the tokens to the sequence. \n",
    "This, now slightly less masked, sequence is then used as the input for the next round, where again, the `k=2` most confident tokens are chosen of the remaining masked targets.\n",
    "\n",
    "A crucial difference of masked models to next-token prediction is that at each inference step we can freely choose the number of tokens `k` to simultaneously decode. \n",
    "Depending on the choice of `k`, this can speed up inference significantly, at little cost to generation performance. \n",
    "For example, see the comparison between raster-scan autoregressive, and masked generation below.\n",
    "Each frame of the gif is one generation step. Autoregressive generation predicts each token one-by-one, while masked models may predict them in parallel.\n",
    "\n",
    "![adsf](https://masked-generative-image-transformer.github.io/imgs/sampling.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ec3ac-cbc1-4ace-808f-a0ed9b88ae6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Overview and tasks\n",
    "\n",
    "To implement nanoMaskGIT, we ask you to complete the subsections below by directly filling in the missing lines in the code base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a0338-c83f-4b7f-b4bf-52e15babc394",
   "metadata": {},
   "source": [
    "#### 2.1.1 Initialize nanoMaskGIT (5 points)\n",
    "\n",
    "We will reuse the exact same Transformer layers and trunk built last week for the nanoGPT model, but this time we will use it to assemble a MaskGIT-like model in `nanofm.models.maskgit.MaskGIT`.\n",
    "It consists of a few operations executed in series. Initialize the following modules in the constructor:\n",
    "1. The discrete input tokens are embedded with an `nn.Embedding` layer. Initialize `self.input_embedding` accordingly, taking into account the vocabulary size.\n",
    "2. On top of that, we add learnable positional embeddings. Initialize `self.positional_embedding` as an `nn.Parameter` containing a randomly initialized Tensor of shape (`max_seq_len`, `dim`).\n",
    "3. To indicate masked-out tokens and provide placeholders to write the targets, initialize `self.mask_token` as an `nn.Parameter` containing a randomly initialized Tensor of shape (`dim`).\n",
    "4. This then gets passed to a Transformer trunk. Initialize `self.trunk` with the trunk you implemented last week.\n",
    "5. Finally we project the trunk output through a LayerNorm and output projection that maps the elements from the Transformer dimension to the vocabulary size (as a one-hot vector per token). Initialize `self.out_norm` and `self.to_logits`. The bias term for `self.to_logits` should always be set to False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea932812-cd3e-4cd6-8d0d-b5ffddd38917",
   "metadata": {},
   "source": [
    "#### 2.1.2 Implement the forward function and loss (10 points)\n",
    "\n",
    "Next, let's implement the `forward_model` function:\n",
    "1. Pass the input tokens through the embedding. \n",
    "2. Given the `mask`, replace these embeddings by the learned `self.mask_token`, wherever `mask == True`.\n",
    "3. Add the positional embedding, pass it through the Transformer trunk, output normalization, and output projection.\n",
    "4. When calling the Transformer trunk, no attention mask needs to be specified. This model performs full self-attention between all masked and non-masked tokens.\n",
    "\n",
    "Finally, we need to compute the cross-entropy loss between the logits and the ground-truth targets. Please complete the `compute_ce_loss` function accordingly, and take into account the ignore_index token. We do not want to compute a loss on non-masked tokens that we pass as input; we only compute it on masked-out tokens that we predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab6ee5-36ad-4b03-b053-1188750696fc",
   "metadata": {},
   "source": [
    "#### 2.1.3 Implement random masking (15 points)\n",
    "\n",
    "As used in the `forward` function, during training we want to randomize the `mask` you just applied to the inputs. \n",
    "For that, please complete the `generate_random_mask` function that should return a random mask where True = masked-out and False = not masked.\n",
    "Each sample in the batch should randomly mask out between 1 and L tokens, where L is the sequence length. \n",
    "When L tokens are masked-out, it means there is no input and all tokens are predicted.\n",
    "When only 1 token is masked-out, it means that all but one token are given as input, and only one is predicted. \n",
    "You should be able to see why we have to have at least one token masked.\n",
    "\n",
    "The returned mask tensor should be of type `torch.BoolTensor`, moved to the same device (GPU) as `seq`, and be of shape (B, L).\n",
    "Note that both the number of mask tokens, as well as the placement of the masks should be sampled completely uniformly at random, for every sample in the batch individually.\n",
    "That means you should not apply and broadcast the same mask to the entire batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533221b-9754-4f89-b8d6-a8ddc105bce7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2.1.4 MaskGIT schedule and generation function (20 points)\n",
    "\n",
    "Now let's implement the generation function. We do that in two steps:\n",
    "\n",
    "First, let's implement a so-called generation schedule in `get_maskgit_schedule`. \n",
    "Its task is to give us a list of number of tokens to unmask at every prediction step. \n",
    "It's quite common to make this a cosine schedule, i.e. where the tokens are unmasked slowly at the beginning, then in the middle many tokens are predicted at once, and finally in the end we ramp down again.\n",
    "Here we will implement a much simpler constant schedule, where the number of unmasked tokens per step is constant.\n",
    "For example, if total_tokens = 17 and num_steps = 8, then the schedule should be: [2, 2, 2, 2, 2, 2, 2, 3]. \n",
    "If the total number of tokens is not divisible by the number of steps, we simply add the remainder to the last step.\n",
    "The `schedule` should be a list of integers of length `num_steps`, where each integer represents the number of tokens to unmask at that step. \n",
    "The sum of the integers in `schedule` should equal `total_tokens`.\n",
    "\n",
    "With the simple schedule implemented, let's use it in the `generate` function. Generation is performed in a loop in the following steps:\n",
    "1. Given the sequence and mask so far, simply pass them through the network to get the logits.\n",
    "2. Then, select the subset of logits that we actually want to predict, i.e. the masked-out tokens.\n",
    "3. Over all these predicted tokens, we only want to keep the most \"confident\" predictions. We select for these by computing the maximum logit value for each token as a proxy. The higher the maximum logit is for a given token, the more \"confident\" it is in its prediction.\n",
    "4. Now, let's select the top-k tokens according to these confidence scores. You get the number of tokens `k` from the generation schedule.\n",
    "5. Sample the token indices from these `k` selected token logits. You should use the `sample_tokens` function from `utils/sampling.py`, and remember to pass the relevant sampling hyperparameters.\n",
    "6. Update the sequence and mask for the next round using the newly sampled tokens and their positions.\n",
    "7. Repeat until the end of the generation schedule, when the sequence is fully unmasked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd64834-6658-4cf1-a88d-aed19c61be1e",
   "metadata": {},
   "source": [
    "### 2.2 Training the model\n",
    "\n",
    "We defined a training config for you in: `cfgs/nanoMaskGIT/mnist_d8w512.yaml`. Please familiarize yourself with all parts.\n",
    "Please don't forget to replace the Weights & Bias entity with your own.\n",
    "\n",
    "On a 1xV100 node, you can start the training like:\n",
    "```\n",
    "OMP_NUM_THREADS=1 torchrun --nproc_per_node=1 run_training.py --config cfgs/nanoMaskGIT/mnist_d8w512.yaml\n",
    "```\n",
    "\n",
    "This training should be pretty fast and only take a few minutes. Because masked image models are harder to overfit, we increased the number of training steps five-fold, compared to nanoGPT. You should reach a final validation loss below 0.57, and your loss curves should look something like the following:\n",
    "\n",
    "<img src=\"./assets/nanoMaskGIT_mnist.png\" alt=\"nanoMaskGIT MNIST loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a95694-bc7d-4ac5-964a-3e0a0e010d5d",
   "metadata": {},
   "source": [
    "### 2.3 Show your loss curves (10 points)\n",
    "\n",
    "Screenshot your loss curves and show them here. Add the image to the `assets` directory and change the path in the markdown. You will get 10 points for reasonable loss curves (similar to the sample loss curves above).\n",
    "\n",
    "<img src=\"./assets/your_loss_curves.png\" alt=\"nanoMaskGIT MNIST loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7567ebf-ca13-4ad5-a16b-8e2c1df0c935",
   "metadata": {},
   "source": [
    "### 2.4 Evaluating the model (10 points)\n",
    "\n",
    "After you completed the training, load the model with the following cell. You may need to adjust the path if you changed it.\n",
    "You will get 10 points if the outputs look reasonable (similar to the sample outputs provided below).\n",
    "\n",
    "Hint: You can also load intermediate safetensors checkpoints to check the progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be3d7773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "Device name: Tesla V100-PCIE-32GB\n",
      "Working directory: /home/rcharif/nano4M-model\n",
      "Checkpoint exists: True\n",
      "Absolute checkpoint path: /home/rcharif/nano4M-model/outputs/nanoMaskGIT/mnist_d8w512/checkpoint-final.safetensors\n",
      "Model loaded to CPU successfully\n",
      "Error loading model: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     31\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel moved to GPU successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Print model configuration\u001b[39;00m\n",
      "File \u001b[0;32m/work/com-304/new_environment/anaconda3/envs/nanofm/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/com-304/new_environment/anaconda3/envs/nanofm/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/work/com-304/new_environment/anaconda3/envs/nanofm/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/work/com-304/new_environment/anaconda3/envs/nanofm/lib/python3.10/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Enable CUDA error checking\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Print CUDA information\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "# Change to the project directory\n",
    "os.chdir('/home/rcharif/nano4M-model')\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "\n",
    "# Verify checkpoint file\n",
    "ckpt_path = './outputs/nanoMaskGIT/mnist_d8w512/checkpoint-final.safetensors'\n",
    "print(\"Checkpoint exists:\", os.path.exists(ckpt_path))\n",
    "print(\"Absolute checkpoint path:\", os.path.abspath(ckpt_path))\n",
    "\n",
    "# Load model to CPU first, then move to GPU\n",
    "try:\n",
    "    # Load model to CPU first\n",
    "    model = load_model_from_safetensors(ckpt_path, device='cpu')\n",
    "    print(\"Model loaded to CPU successfully\")\n",
    "    \n",
    "    # Move to GPU carefully\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        model = model.to(device)\n",
    "        print(\"Model moved to GPU successfully\")\n",
    "    \n",
    "    # Print model configuration\n",
    "    print(\"\\nModel configuration:\")\n",
    "    print(f\"Vocab size: {model.input_embedding.num_embeddings}\")\n",
    "    print(f\"Sequence length: {model.positional_embedding.shape[0]}\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    print(f\"Error loading model: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f1148c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/rcharif/nano4M-model\n",
      "New working directory: /home/rcharif/nano4M-model\n",
      "File exists: True\n",
      "Absolute path: /home/rcharif/nano4M-model/outputs/nanoMaskGIT/mnist_d8w512/checkpoint-final.safetensors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbsolute path:\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(ckpt_path))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Now try loading the model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_from_safetensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nano4M-model/nanofm/utils/checkpoint.py:207\u001b[0m, in \u001b[0;36mload_model_from_safetensors\u001b[0;34m(ckpt_path, device, to_eval)\u001b[0m\n\u001b[1;32m    205\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m to_eval:\n\u001b[1;32m    209\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/work/com-304/new_environment/anaconda3/envs/nanofm/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/com-304/new_environment/anaconda3/envs/nanofm/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/work/com-304/new_environment/anaconda3/envs/nanofm/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/work/com-304/new_environment/anaconda3/envs/nanofm/lib/python3.10/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Change to the project directory\n",
    "os.chdir('/home/rcharif/nano4M-model')\n",
    "\n",
    "# Print the new working directory\n",
    "print(\"New working directory:\", os.getcwd())\n",
    "\n",
    "# Verify file exists\n",
    "ckpt_path = './outputs/nanoMaskGIT/mnist_d8w512/checkpoint-final.safetensors'\n",
    "print(\"File exists:\", os.path.exists(ckpt_path))\n",
    "print(\"Absolute path:\", os.path.abspath(ckpt_path))\n",
    "\n",
    "# Now try loading the model\n",
    "model = load_model_from_safetensors(ckpt_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fe6e1bb-d198-4f3f-93b1-aed5cf5eb7aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./outputs/nanoMaskGIT/mnist_d8w512/checkpoint-final.safetensors\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_from_safetensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mget_num_params()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mM parameters\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/nano4M-model/nanofm/utils/checkpoint.py:207\u001b[0m, in \u001b[0;36mload_model_from_safetensors\u001b[0;34m(ckpt_path, device, to_eval)\u001b[0m\n\u001b[1;32m    205\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m to_eval:\n\u001b[1;32m    209\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/work/com-304/new_environment/anaconda3/envs/nanofm/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/com-304/new_environment/anaconda3/envs/nanofm/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/work/com-304/new_environment/anaconda3/envs/nanofm/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/work/com-304/new_environment/anaconda3/envs/nanofm/lib/python3.10/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = './outputs/nanoMaskGIT/mnist_d8w512/checkpoint-final.safetensors'\n",
    "model = load_model_from_safetensors(ckpt_path, device=device)\n",
    "print(f'{model.get_num_params() / 10**6}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f899e5d6-dd87-4c07-8f40-351c49ff3524",
   "metadata": {},
   "source": [
    "Let's plot some class-conditional generations! We seed the generation by providing the first token, whose index is equal to the number we'd like to generate.\n",
    "For that token, we initialize the mask with `False`, i.e. indicating that that token is given as input, i.e. not masked. \n",
    "The rest of the tokens are masked, and it does not matter what value they have in the `seq` tensor, as they are overwritten by the learnable mask token in the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e298bb3-535f-4f70-90f4-0e8af217c0c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 3\u001b[0m seq \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m50\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      5\u001b[0m seq[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m label\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "label = 5\n",
    "\n",
    "seq = torch.zeros(50, dtype=torch.long, device=device)\n",
    "mask = torch.ones(50, dtype=torch.bool, device=device)\n",
    "seq[0] = label\n",
    "mask[0] = False\n",
    "\n",
    "output = model.generate(seq, mask, num_steps=8, temp=1.0, top_p=0.9, return_history=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f7d1cde-f9ae-4bc8-b66f-cc6a89346721",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reconst \u001b[38;5;241m=\u001b[39m \u001b[43mdetokenize_MNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccount_for_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(reconst[\u001b[38;5;241m0\u001b[39m], cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray_r\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/nano4M-model/nanofm/data/vision/tokenized_mnist.py:68\u001b[0m, in \u001b[0;36mdetokenize_MNIST\u001b[0;34m(imgs_tokenized, patch_size, account_for_labels)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdetokenize_MNIST\u001b[39m(imgs_tokenized, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, account_for_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    Reconstructs MNIST images from tokenized representations.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m        Tensor: Reconstructed images with shape [B, H, W].\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     imgs_tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mimgs_tokenized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m account_for_labels:\n\u001b[1;32m     70\u001b[0m         imgs_tokenized \u001b[38;5;241m=\u001b[39m imgs_tokenized[:, \u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "reconst = detokenize_MNIST(output, patch_size=2, account_for_labels=True).cpu()\n",
    "plt.imshow(reconst[0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88da2fcd-f7cc-479a-8124-c7c52f0e0332",
   "metadata": {},
   "source": [
    "Let's now generate 10 random samples for all 10 classes. Most should look quite reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a37a578-6b9f-4a62-bc36-bb9345c3b38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApcAAAKXCAYAAADXbGGRAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUItJREFUeJzt3U9oVGuC//9PxR9WbIkZRG4lFRMMjRs3BrQTwhVamYBkYevq6srgRhjMgAQMfUETCEIGB4a004Esxc1MuA2md9mE4bpIJqJ9ZRZDB6XDl8JQ1fbCRMNYgjm/RXeqqyqVP0dPJSf1eb9ASNWpk3reFoSHU+c8JxEEQSAAAAAgAnV7PQAAAADUDiaXAAAAiAyTSwAAAESGySUAAAAiw+QSAAAAkWFyCQAAgMgwuQQAAEBkmFwCAAAgMkwuAQAAEBkmlwAAAIhM1SaX4+PjOnHihOrr69XV1aVnz55V660AAAAQE4lq3Ft8cnJS169f18TEhLq6ujQ2NqYffvhBCwsL+uabb7bcd21tTUtLS2poaFAikYh6aHsiCAK9f/9e6XRadXWbz+dpr512127Jt921W6Ldsd21W/Jt32n3+osj19nZGdy6davw+PPnz0E6nQ5GR0e33TeTyQSSavJfJpOh3azdtdu53bWbds92127n9u26gyAI/j9F7NOnT3rx4oW+//77wnN1dXXq6enR3Nzchtfn83nl8/nC4+BvB1IzmYyOHDkiSWpsbCzZZ3l5ufDzVtuquW/59u32laSGhoaSx7TXfrtrt+Tb7tot0V7Mpd21W/JtL++uaNvpZ0hv3rwJJAWzs7Mlz9+5cyfo7Ozc8Prh4eGKM+Pl5eXCa8q3FdtqWzX3Ld++3b7lTbR7tLt2O7e7dtPu2e7a7dxe3l1J5OdcLi0tqaWlRbOzs+ru7i48Pzg4qB9//FHz8/Mlry+f6a+srKi1tVXLy8uFmX75uQrFQ95qWzX3Ld++3b6SSpok2h3aXbsl33bXbol2x3bXbsm3vby7ksi/Fj927JgOHDigXC5X8nwul1NTU9OG1yeTSSWTyQ3PFx+G/Zr/hGrtW759q20rKysVD6nTXrvtrt2Sb7trt0S7Y7trt+Tbvll3JZEvRXTw4EGdOXNGMzMzhefW1tY0MzNTciQTAAAAtSfyI5eSNDAwoL6+Pp09e1adnZ0aGxvT6uqqbty4UY23AwAAQExUZXJ59epVvX37VkNDQ8pms+ro6ND09LRSqVQ13g4AAAAxUZXJpST19/erv7//i/evtRNgw6Ddr921W/Jtd+2WaHdsd+2WPNu5tzgAAAAiw+QSAAAAkana1+Jfq1Yu3ZfCXb4v0b6TMcS93bVb8m137ZZod2x37ZZ82/d0KSIAAAD4YnIJAACAyDC5BAAAQGRie86l46X762j3a3ftlnzbXbsl2h3bXbslz3aOXAIAACAyTC4BAAAQGSaXAAAAiExsz7mslXWhJN81sSTfdtduybfdtVui3bHdtVvybWedSwAAAOwJJpcAAACIDJNLAAAARCa251w6rgu1jna/dtduybfdtVui3bHdtVvybOfIJQAAACLD5BIAAACRie3X4rVy6b7ku2yB5Nvu2i35trt2S7Q7trt2S77tLEUEAACAPcHkEgAAAJFhcgkAAIDIxPacS8dL99fR7tfu2i35trt2S7Q7trt2S57tHLkEAABAZJhcAgAAIDJMLgEAABCZ2J5zWSvrQkm+a2JJvu2u3ZJvu2u3RLtju2u35NvOOpcAAADYE6Enl0+fPtWlS5eUTqeVSCQ0NTVVsj0IAg0NDam5uVmHDh1ST0+PXr16FdV4AQAAEGOhJ5erq6s6ffq0xsfHK25/8OCBHj58qImJCc3Pz+vw4cO6ePGiPn78+NWDBQAAQLyFPueyt7dXvb29FbcFQaCxsTHdvXtXly9fliQ9fvxYqVRKU1NTunbt2o7fx3FdqHW0+7W7dku+7a7dEu2O7a7dkmd7pBf0LC4uKpvNqqenp/BcY2Ojurq6NDc3V3Fymc/nlc/nC49XVlaiHFKs0e7X7tot+ba7dku0O7a7dkve7eUivaAnm81KklKpVMnzqVSqsK3c6OioGhsbC/9aW1ujHFKs0e7X7tot+ba7dku0O7a7dkve7eUSQfnxzzA7JxJ68uSJrly5IkmanZ3Vt99+q6WlJTU3Nxde99133ymRSGhycnLD76g0029tba3Jw8jFTRLtDu2u3ZJvu2u3RLtju2u35Nte3l1JpF+LNzU1SZJyuVzJ5DKXy6mjo6PiPslkUslkcsPztbIulLT52lC01267a7fk2+7aLdHu2O7aLfm279k6l+3t7WpqatLMzEzhuZWVFc3Pz6u7uzvKtwIAAEAMhT5y+eHDB71+/brweHFxUS9fvtTRo0fV1tam27dv6/79+zp58qTa29t17949pdPpwlfnAAAAqF2hJ5fPnz/XhQsXCo8HBgYkSX19fXr06JEGBwe1urqqmzdv6t27dzp37pymp6dVX18f6n1q7RyFMGj3a3ftlnzbXbsl2h3bXbslz/bQk8vz589veOPyQYyMjGhkZCT0YAAAALC/cW9xAAAARIbJJQAAACIT6VJEUaqVS/elcJfvS7TvZAxxb3ftlnzbXbsl2h3bXbsl3/Y9W4oIAAAA3phcAgAAIDKx+1q80pXoW938fbsbw1dr3zC/e/3nra6y32w77dUdX7V+t2t38c9u7a7dxT/Tvrlaa3ftLv7ZrX2n3esvipVMJhNIqsl/mUyGdrN2127ndtdu2j3bXbud27frDoIgSATBTqagu2dtbU1LS0sKgkBtbW3KZDLb3iB9N63fiD7MuIIg0Pv375VOp1VXt/mZCHFu/5JuKVz7wsKCTp06Fatuyfczl3zbXbsl33b+xvl95pJvezW7pRh+LV5XV6fjx48XDr8eOXIkNh9GsbDj2skVVvuh/UvGtNP2lpaWL36P3eD6mUu+7a7dkm87f+P8PnPJt70a3RIX9AAAACBCTC4BAAAQmdhOLpPJpIaHh5VMJvd6KCV2Y1xxbHftlmh3bHftlnzbXbsl2h3bqz2m2F3QAwAAgP0rtkcuAQAAsP9UbXI5Pj6uEydOqL6+Xl1dXXr27Fm13goAAAAxUZWvxScnJ3X9+nVNTEyoq6tLY2Nj+uGHH7SwsKBvvvlmy33X14VqaGjYcPP0/Srsmli07/92127Jt921W6Ldsd21W/JtD7POZVXu0NPZ2RncunWr8Pjz589BOp0ORkdHt923Vle0l/Fq/s7trt3O7a7dtHu2u3Y7t+/kDj2RL6L+6dMnvXjxQt9//33hubq6OvX09Ghubm7D6/P5vPL5fOFx8LcDqcWrxpcv2rm8vFz4eatt1dy3fPt2+0pSQ0NDyWPaa7/dtVvybXftlmgv5tLu2i35tpd3V7Tt9DOkN2/eBJKC2dnZkufv3LkTdHZ2bnj98PBwxZnx8vJy4TXl24ptta2a+5Zv327f8ibaPdpdu53bXbtp92x37XZuL++uJPJzLpeWltTS0qLZ2Vl1d3cXnh8cHNSPP/6o+fn5kteXz/TX73dZrHyIxecubLWtmvuWb99q28rKihobG7W8vFxymyXaa7fdtVvybXftlmh3bHftlnzbN+uuJPKvxY8dO6YDBw4ol8uVPJ/L5dTU1LTh9clkMlYLi+4m2v3aXbsl33bXbol2x3bXbsm7vVzkSxEdPHhQZ86c0czMTOG5tbU1zczMlBzJBAAAQO2J/MilJA0MDKivr09nz55VZ2enxsbGtLq6qhs3blTj7QAAABATVZlcXr16VW/fvtXQ0JCy2aw6Ojo0PT2tVCq1499R/J3+l54XUc19y7dvt28YtPu1u3ZLvu2u3RLtju2u3ZJne1Uml5LU39+v/v7+av16AAAAxBD3FgcAAEBkqnbk8msVL9q5ny/dl/5++f5O0b79GOLe7tot+ba7dku0O7a7dku+7WG6OXIJAACAyDC5BAAAQGSYXAIAACAysT3n0vHS/XW0+7W7dku+7a7dEu2O7a7dkmc7Ry4BAAAQGSaXAAAAiAyTSwAAAEQmtudc1sq6UJLvmliSb7trt+Tb7tot0e7Y7tot+bazziUAAAD2BJNLAAAARIbJJQAAACIT23MuHdeFWke7X7trt+Tb7tot0e7Y7totebZz5BIAAACRYXIJAACAyMT2a/FauXRf8l22QPJtd+2WfNtduyXaHdtduyXfdpYiAgAAwJ5gcgkAAIDIMLkEAABAZGJ7zqXjpfvraPdrd+2WfNtduyXaHdtduyXPdo5cAgAAIDJMLgEAABAZJpcAAACITGzPuayVdaEk3zWxJN92127Jt921W6Ldsd21W/JtZ51LAAAA7InQk8unT5/q0qVLSqfTSiQSmpqaKtkeBIGGhobU3NysQ4cOqaenR69evYpqvAAAAIix0JPL1dVVnT59WuPj4xW3P3jwQA8fPtTExITm5+d1+PBhXbx4UR8/fvzqwQIAACDeQp9z2dvbq97e3orbgiDQ2NiY7t69q8uXL0uSHj9+rFQqpampKV27dm3H7+O4LtQ62v3aXbsl33bXbol2x3bXbsmzPdILehYXF5XNZtXT01N4rrGxUV1dXZqbm6s4uczn88rn84XHKysrUQ4p1mj3a3ftlnzbXbsl2h3bXbsl7/ZykV7Qk81mJUmpVKrk+VQqVdhWbnR0VI2NjYV/ra2tUQ4p1mj3a3ftlnzbXbsl2h3bXbsl7/ZyiaD8+GeYnRMJPXnyRFeuXJEkzc7O6ttvv9XS0pKam5sLr/vuu++USCQ0OTm54XdUmum3trbW5GHk4iaJdod2127Jt921W6Ldsd21W/JtL++uJNKvxZuamiRJuVyuZHKZy+XU0dFRcZ9kMqlkMrnh+VpZF0rafG0o2mu33bVb8m137ZZod2x37ZZ82/dsncv29nY1NTVpZmam8NzKyorm5+fV3d0d5VsBAAAghkIfufzw4YNev35deLy4uKiXL1/q6NGjamtr0+3bt3X//n2dPHlS7e3tunfvntLpdOGrcwAAANSu0JPL58+f68KFC4XHAwMDkqS+vj49evRIg4ODWl1d1c2bN/Xu3TudO3dO09PTqq+vD/U+tXaOQhi0+7W7dku+7a7dEu2O7a7dkmd76Mnl+fPnN7xx+SBGRkY0MjISejAAAADY37i3OAAAACLD5BIAAACRiXQpoijVyqX7UrjL9yXadzKGuLe7dku+7a7dEu2O7a7dkm/7ni1FBAAAAG9MLgEAABCZ2H0tXulK9K1u/r7djeGrtW+Y373+81ZX2W+2nfbqjq9av9u1u/hnt3bX7uKfad9crbW7dhf/7Na+0+71F8VKJpMJJNXkv0wmQ7tZu2u3c7trN+2e7a7dzu3bdQdBECSCYCdT0N2ztrampaUlBUGgtrY2ZTKZbW+QvpvWb0QfZlxBEOj9+/dKp9Oqq9v8TIQ4t39JtxSufWFhQadOnYpVt+T7mUu+7a7dkm87f+P8PnPJt72a3VIMvxavq6vT8ePHC4dfjxw5EpsPo1jYce3kCqv90P4lY9ppe0tLyxe/x25w/cwl33bXbsm3nb9xfp+55NtejW6JC3oAAAAQISaXAAAAiExsJ5fJZFLDw8NKJpN7PZQSuzGuOLa7dku0O7a7dku+7a7dEu2O7dUeU+wu6AEAAMD+FdsjlwAAANh/qja5HB8f14kTJ1RfX6+uri49e/asWm8FAACAmKjK1+KTk5O6fv26JiYm1NXVpbGxMf3www9aWFjQN998s+W+6+tCNTQ0bLh5+n4Vdk0s2vd/u2u35Nvu2i3R7tju2i35todZ57Iqd+jp7OwMbt26VXj8+fPnIJ1OB6Ojo9vuW6sr2st4NX/ndtdu53bXbto92127ndt3coeeyBdR//Tpk168eKHvv/++8FxdXZ16eno0Nze34fX5fF75fL7wOPjbgdTiVePLF+1cXl4u/LzVtmruW759u30lqaGhoeQx7bXf7tot+ba7dku0F3Npd+2WfNvLuyvadvoZ0ps3bwJJwezsbMnzd+7cCTo7Oze8fnh4uOLMeHl5ufCa8m3FttpWzX3Lt2+3b3kT7R7trt3O7a7dtHu2u3Y7t5d3VxL5OZdLS0tqaWnR7Oysuru7C88PDg7qxx9/1Pz8fMnry2f66/e7XF5eLsz0y89VKB7yVtuquW/59u32lVTSJNHu0O7aLfm2u3ZLtDu2u3ZLvu3l3ZVE/rX4sWPHdODAAeVyuZLnc7mcmpqaNrw+mUxWXMSz+DDs1/wnVGvf8u1bbVtZWal4SJ322m137ZZ82127Jdod2127Jd/2zboriXwpooMHD+rMmTOamZkpPLe2tqaZmZmSI5kAAACoPZEfuZSkgYEB9fX16ezZs+rs7NTY2JhWV1d148aNarwdAAAAYqIqk8urV6/q7du3GhoaUjabVUdHh6anp5VKparxdgAAAIiJqkwuJam/v1/9/f1fvH+tnQAbBu1+7a7dkm+7a7dEu2O7a7fk2c69xQEAABAZJpcAAACITNW+Fv9atXLpvhTu8n2J9p2MIe7trt2Sb7trt0S7Y7trt+TbvqdLEQEAAMAXk0sAAABEhsklAAAAIhPbcy4dL91fR7tfu2u35Nvu2i3R7tju2i15tnPkEgAAAJFhcgkAAIDIMLkEAABAZGJ7zmWtrAsl+a6JJfm2u3ZLvu2u3RLtju2u3ZJvO+tcAgAAYE8wuQQAAEBkmFwCAAAgMrE959JxXah1tPu1u3ZLvu2u3RLtju2u3ZJnO0cuAQAAEBkmlwAAAIhMbL8Wr5VL9yXfZQsk33bXbsm33bVbot2x3bVb8m1nKSIAAADsCSaXAAAAiAyTSwAAAEQmtudcOl66v452v3bXbsm33bVbot2x3bVb8mznyCUAAAAiw+QSAAAAkWFyCQAAgMjE9pzLWlkXSvJdE0vybXftlnzbXbsl2h3bXbsl33bWuQQAAMCeCD25fPr0qS5duqR0Oq1EIqGpqamS7UEQaGhoSM3NzTp06JB6enr06tWrqMYLAACAGAs9uVxdXdXp06c1Pj5ecfuDBw/08OFDTUxMaH5+XocPH9bFixf18ePHrx4sAAAA4i30OZe9vb3q7e2tuC0IAo2Njenu3bu6fPmyJOnx48dKpVKamprStWvXdvw+jutCraPdr921W/Jtd+2WaHdsd+2WPNsjvaBncXFR2WxWPT09hecaGxvV1dWlubm5ipPLfD6vfD5feLyyshLlkGKNdr92127Jt921W6Ldsd21W/JuLxfpBT3ZbFaSlEqlSp5PpVKFbeVGR0fV2NhY+Nfa2hrlkGKNdr92127Jt921W6Ldsd21W/JuL5cIyo9/htk5kdCTJ0905coVSdLs7Ky+/fZbLS0tqbm5ufC67777TolEQpOTkxt+R6WZfmtra00eRi5ukmh3aHftlnzbXbsl2h3bXbsl3/by7koi/Vq8qalJkpTL5Uoml7lcTh0dHRX3SSaTSiaTG56vlXWhpM3XhqK9dttduyXfdtduiXbHdtduybd9z9a5bG9vV1NTk2ZmZgrPraysaH5+Xt3d3VG+FQAAAGIo9JHLDx8+6PXr14XHi4uLevnypY4ePaq2tjbdvn1b9+/f18mTJ9Xe3q579+4pnU4XvjoHAABA7Qo9uXz+/LkuXLhQeDwwMCBJ6uvr06NHjzQ4OKjV1VXdvHlT796907lz5zQ9Pa36+vpQ71Nr5yiEQbtfu2u35Nvu2i3R7tju2i15toeeXJ4/f37DG5cPYmRkRCMjI6EHAwAAgP2Ne4sDAAAgMkwuAQAAEJlIlyKKUq1cui+Fu3xfon0nY4h7u2u35Nvu2i3R7tju2i35tu/ZUkQAAADwxuQSAAAAkYnd1+KVrkTf6ubv290Yvlr7hvnd6z9vdZX9Zttpr+74qvW7XbuLf3Zrd+0u/pn2zdVau2t38c9u7TvtXn9RrGQymUBSTf7LZDK0m7W7dju3u3bT7tnu2u3cvl13EARBIgh2MgXdPWtra1paWlIQBGpra1Mmk9n2Bum7af1G9GHGFQSB3r9/r3Q6rbq6zc9EiHP7l3RL4doXFhZ06tSpWHVLvp+55Nvu2i35tvM3zu8zl3zbq9ktxfBr8bq6Oh0/frxw+PXIkSOx+TCKhR3XTq6w2g/tXzKmnba3tLR88XvsBtfPXPJtd+2WfNv5G+f3mUu+7dXolrigBwAAABFicgkAAIDIxHZymUwmNTw8rGQyuddDKbEb44pju2u3RLtju2u35Nvu2i3R7the7THF7oIeAAAA7F+xPXIJAACA/adqk8vx8XGdOHFC9fX16urq0rNnz6r1VgAAAIiJqnwtPjk5qevXr2tiYkJdXV0aGxvTDz/8oIWFBX3zzTdb7ru+LlRDQ8OGm6fvV2HXxKJ9/7e7dku+7a7dEu2O7a7dkm97mHUuq3KHns7OzuDWrVuFx58/fw7S6XQwOjq67b61uqK9jFfzd2537XZud+2m3bPdtdu5fSd36Il8EfVPnz7pxYsX+v777wvP1dXVqaenR3Nzcxten8/nlc/nC4+Dvx1ILV41vnzRzuXl5cLPW22r5r7l27fbV5IaGhpKHtNe++2u3ZJvu2u3RHsxl3bXbsm3vby7om2nnyG9efMmkBTMzs6WPH/nzp2gs7Nzw+uHh4crzoyXl5cLrynfVmyrbdXct3z7dvuWN9Hu0e7a7dzu2k27Z7trt3N7eXclkZ9zubS0pJaWFs3Ozqq7u7vw/ODgoH788UfNz8+XvL58pr9+v8vl5eXCTL/8XIXiIW+1rZr7lm/fbl9JJU0S7Q7trt2Sb7trt0S7Y7trt+TbXt5dSeRfix87dkwHDhxQLpcreT6Xy6mpqWnD65PJZMVFPIsPw37Nf0K19i3fvtW2lZWViofUaa/ddtduybfdtVui3bHdtVvybd+su5LIlyI6ePCgzpw5o5mZmcJza2trmpmZKTmSCQAAgNoT+ZFLSRoYGFBfX5/Onj2rzs5OjY2NaXV1VTdu3KjG2wEAACAmqjK5vHr1qt6+fauhoSFls1l1dHRoenpaqVSqGm8HAACAmKjK5FKS+vv71d/f/8X719oJsGHQ7tfu2i35trt2S7Q7trt2S57t3FscAAAAkWFyCQAAgMhU7Wvxr1Url+5L4S7fl2jfyRji3u7aLfm2u3ZLtDu2u3ZLvu17uhQRAAAAfDG5BAAAQGSYXAIAACAysT3n0vHS/XW0+7W7dku+7a7dEu2O7a7dkmc7Ry4BAAAQGSaXAAAAiAyTSwAAAEQmtudc1sq6UJLvmliSb7trt+Tb7tot0e7Y7tot+bazziUAAAD2BJNLAAAARIbJJQAAACIT23MuHdeFWke7X7trt+Tb7tot0e7Y7totebZz5BIAAACRYXIJAACAyMT2a/FauXRf8l22QPJtd+2WfNtduyXaHdtduyXfdpYiAgAAwJ5gcgkAAIDIMLkEAABAZGJ7zqXjpfvraPdrd+2WfNtduyXaHdtduyXPdo5cAgAAIDJMLgEAABAZJpcAAACITGzPuayVdaEk3zWxJN92127Jt921W6Ldsd21W/Jtr+o6l0+fPtWlS5eUTqeVSCQ0NTW1YRBDQ0Nqbm7WoUOH1NPTo1evXoV9GwAAAOxDoSeXq6urOn36tMbHxytuf/DggR4+fKiJiQnNz8/r8OHDunjxoj5+/PjVgwUAAEC8hf5avLe3V729vRW3BUGgsbEx3b17V5cvX5YkPX78WKlUSlNTU7p27drXjRYAAACxFuk5l4uLi8pms+rp6Sk819jYqK6uLs3NzYWaXDquC7WOdr92127Jt921W6Ldsd21W/Jsj3Rymc1mJUmpVKrk+VQqVdhWLp/PK5/PFx6vrKxEOaRYo92v3bVb8m137ZZod2x37Za828vt+VJEo6OjamxsLPxrbW3d6yHtGtr92l27Jd92126Jdsd2127Ju71cIig//hlm50RCT5480ZUrVyRJf/rTn/Tzn/9cP/30kzo6Ogqv++Uvf6mOjg795je/2fA7Ks30W1tba/IwcnGTRLtDu2u35Nvu2i3R7tju2i35tpd3VxLp1+Lt7e1qamrSzMxMYXK5srKi+fl5/dM//VPFfZLJpJLJ5Ibna2VdKGnztaFor912127Jt921W6Ldsd21W/JtD7POZejJ5YcPH/T69evC48XFRb18+VJHjx5VW1ubbt++rfv37+vkyZNqb2/XvXv3lE6nC0c3AQAAULtCTy6fP3+uCxcuFB4PDAxIkvr6+vTo0SMNDg5qdXVVN2/e1Lt373Tu3DlNT0+rvr4+ulEDAAAglkJPLs+fP7/h8GqxRCKhkZERjYyMfNXAau0chTBo92t37ZZ82127Jdod2127Jc/2Pb9aHAAAALWDySUAAAAiw+QSAAAAkYl0KaIo1cql+1K4y/cl2ncyhri3u3ZLvu2u3RLtju2u3ZJve5hujlwCAAAgMkwuAQAAEJnYfS1efuhW2vrm79vdGL5a+4b53es/V2orRvuXvcfXjK9av9u1u/hnt3bX7uKfad9crbW7dhf/7Na+0+71F8VKJpMJJNXkv0wmQ7tZu2u3c7trN+2e7a7dzu3bdQdBECSCYCdT0N2ztrampaUlBUGgtrY2ZTKZbW+QvpvWb0QfZlxBEOj9+/dKp9Oqq9v8TIQ4t39JtxSufWFhQadOnYpVt+T7mUu+7a7dkm87f+P8PnPJt72a3VIMvxavq6vT8ePHC4dfjxw5EpsPo1jYce3kCqv90P4lY9ppe0tLyxe/x25w/cwl33bXbsm3nb9xfp+55NtejW6JC3oAAAAQISaXAAAAiExsJ5fJZFLDw8NKJpN7PZQSuzGuOLa7dku0O7a7dku+7a7dEu2O7dUeU+wu6AEAAMD+VbUjl+Pj4zpx4oTq6+vV1dWlZ8+eVeutAAAAEBNVmVxOTk5qYGBAw8PD+sMf/qDTp0/r4sWL+vOf/1yNtwMAAEBMVOVr8a6uLv3iF7/Qb3/7W0l/XeuptbVV//zP/6xf//rXW+67vi5UQ0PDhpun71dh18Siff+3u3ZLvu2u3RLtju2u3ZJve5h1LiO/Q08+nw8OHDgQPHnypOT569evB7/61a+23b9WV7SX8Wr+zu2u3c7trt20e7a7dju37+QOPZEvov6Xv/xFnz9/ViqVKnk+lUrpj3/844bX5/N55fP5wuPgbwdSi1eNL1+0c3l5ufDzVtuquW/59u32laSGhoaSx7TXfrtrt+Tb7tot0V7Mpd21W/JtL++uaNvpZ0hv3rwJJAWzs7Mlz9+5cyfo7Ozc8Prh4eGKM+Pl5eXCa8q3FdtqWzX3Ld++3b7lTbR7tLt2O7e7dtPu2e7a7dxe3l1J5Odcfvr0ST/72c/0u9/9TleuXCk839fXp3fv3un3v/99yevLZ/rr97ssVj7E4nMXttpWzX3Lt2+1bWVlRY2NjVpeXi65zRLttdvu2i35trt2S7Q7trt2S77tm3VXEvnX4gcPHtSZM2c0MzNTmFyura1pZmZG/f39G16fTCZjtbDobqLdr921W/Jtd+2WaHdsd+2WvNvLRT65lKSBgQH19fXp7Nmz6uzs1NjYmFZXV3Xjxo1qvB0AAABioiqTy6tXr+rt27caGhpSNptVR0eHpqenN1zkAwAAgNpSlcmlJPX391f8Gnynir/T/9LzIqq5b/n27fYNg3a/dtduybfdtVui3bHdtVvybK/a7R8BAADgh8klAAAAIlO1r8W/VvGinfv50n3p75fv7xTt248h7u2u3ZJvu2u3RLtju2u35NseppsjlwAAAIgMk0sAAABEhsklAAAAIhPbcy4dL91fR7tfu2u35Nvu2i3R7tju2i15tnPkEgAAAJFhcgkAAIDIMLkEAABAZGJ7zmWtrAsl+a6JJfm2u3ZLvu2u3RLtju2u3ZJvO+tcAgAAYE8wuQQAAEBkmFwCAAAgMrE959JxXah1tPu1u3ZLvu2u3RLtju2u3ZJnO0cuAQAAEBkmlwAAAIhMbL8Wr5VL9yXfZQsk33bXbsm33bVbot2x3bVb8m1nKSIAAADsCSaXAAAAiAyTSwAAAEQmtudcOl66v452v3bXbsm33bVbot2x3bVb8mznyCUAAAAiw+QSAAAAkWFyCQAAgMjE9pzLWlkXSvJdE0vybXftlnzbXbsl2h3bXbsl33bWuQQAAMCeCD25fPr0qS5duqR0Oq1EIqGpqamS7UEQaGhoSM3NzTp06JB6enr06tWrqMYLAACAGAs9uVxdXdXp06c1Pj5ecfuDBw/08OFDTUxMaH5+XocPH9bFixf18ePHrx4sAAAA4i30OZe9vb3q7e2tuC0IAo2Njenu3bu6fPmyJOnx48dKpVKamprStWvXdvw+jutCraPdr921W/Jtd+2WaHdsd+2WPNsjvaBncXFR2WxWPT09hecaGxvV1dWlubm5ipPLfD6vfD5feLyyshLlkGKNdr92127Jt921W6Ldsd21W/JuLxfpBT3ZbFaSlEqlSp5PpVKFbeVGR0fV2NhY+Nfa2hrlkGKNdr92127Jt921W6Ldsd21W/JuL5cIyo9/htk5kdCTJ0905coVSdLs7Ky+/fZbLS0tqbm5ufC67777TolEQpOTkxt+R6WZfmtra00eRi5ukmh3aHftlnzbXbsl2h3bXbsl3/by7koi/Vq8qalJkpTL5Uoml7lcTh0dHRX3SSaTSiaTG56vlXWhpM3XhqK9dttduyXfdtduiXbHdtduybd9z9a5bG9vV1NTk2ZmZgrPraysaH5+Xt3d3VG+FQAAAGIo9JHLDx8+6PXr14XHi4uLevnypY4ePaq2tjbdvn1b9+/f18mTJ9Xe3q579+4pnU4XvjoHAABA7Qo9uXz+/LkuXLhQeDwwMCBJ6uvr06NHjzQ4OKjV1VXdvHlT796907lz5zQ9Pa36+vpQ71Nr5yiEQbtfu2u35Nvu2i3R7tju2i15toeeXJ4/f37DG5cPYmRkRCMjI6EHAwAAgP2Ne4sDAAAgMkwuAQAAEJlIlyKKUq1cui+Fu3xfon0nY4h7u2u35Nvu2i3R7tju2i35tu/ZUkQAAADwxuQSAAAAkYnd1+KVrkTf6ubv290Yvlr7hvnd6z9vdZX9Zttpr+74qvW7XbuLf3Zrd+0u/pn2zdVau2t38c9u7TvtXn9RrGQymUBSTf7LZDK0m7W7dju3u3bT7tnu2u3cvl13EARBIgh2MgXdPWtra1paWlIQBGpra1Mmk9n2Bum7af1G9GHGFQSB3r9/r3Q6rbq6zc9EiHP7l3RL4doXFhZ06tSpWHVLvp+55Nvu2i35tvM3zu8zl3zbq9ktxfBr8bq6Oh0/frxw+PXIkSOx+TCKhR3XTq6w2g/tXzKmnba3tLR88XvsBtfPXPJtd+2WfNv5G+f3mUu+7dXolrigBwAAABFicgkAAIDIxHZymUwmNTw8rGQyuddDKbEb44pju2u3RLtju2u35Nvu2i3R7the7THF7oIeAAAA7F+xPXIJAACA/adqk8vx8XGdOHFC9fX16urq0rNnz6r1VgAAAIiJqnwtPjk5qevXr2tiYkJdXV0aGxvTDz/8oIWFBX3zzTdb7ru+LlRDQ8OGm6fvV2HXxKJ9/7e7dku+7a7dEu2O7a7dkm97mHUuq3KHns7OzuDWrVuFx58/fw7S6XQwOjq67b61uqK9jFfzd2537XZud+2m3bPdtdu5fSd36Il8EfVPnz7pxYsX+v777wvP1dXVqaenR3Nzcxten8/nlc/nC4+Dvx1ILV41vnzRzuXl5cLPW22r5r7l27fbV5IaGhpKHtNe++2u3ZJvu2u3RHsxl3bXbsm3vby7om2nnyG9efMmkBTMzs6WPH/nzp2gs7Nzw+uHh4crzoyXl5cLrynfVmyrbdXct3z7dvuWN9Hu0e7a7dzu2k27Z7trt3N7eXclkZ9zubS0pJaWFs3Ozqq7u7vw/ODgoH788UfNz8+XvL58pr9+v8vl5eXCTL/8XIXiIW+1rZr7lm/fbl9JJU0S7Q7trt2Sb7trt0S7Y7trt+TbXt5dSeRfix87dkwHDhxQLpcreT6Xy6mpqWnD65PJZMVFPIsPw37Nf0K19i3fvtW2lZWViofUaa/ddtduybfdtVui3bHdtVvybd+su5LIlyI6ePCgzpw5o5mZmcJza2trmpmZKTmSCQAAgNoT+ZFLSRoYGFBfX5/Onj2rzs5OjY2NaXV1VTdu3KjG2wEAACAmqjK5vHr1qt6+fauhoSFls1l1dHRoenpaqVSqGm8HAACAmKjK5FKS+vv71d/f/8X719oJsGHQ7tfu2i35trt2S7Q7trt2S57t3FscAAAAkWFyCQAAgMhU7Wvxr1Url+5L4S7fl2jfyRji3u7aLfm2u3ZLtDu2u3ZLvu17uhQRAAAAfDG5BAAAQGSYXAIAACAysT3n0vHS/XW0+7W7dku+7a7dEu2O7a7dkmc7Ry4BAAAQGSaXAAAAiAyTSwAAAEQmtudc1sq6UJLvmliSb7trt+Tb7tot0e7Y7tot+bazziUAAAD2BJNLAAAARIbJJQAAACIT23MuHdeFWke7X7trt+Tb7tot0e7Y7totebZz5BIAAACRYXIJAACAyMT2a/FauXRf8l22QPJtd+2WfNtduyXaHdtduyXfdpYiAgAAwJ5gcgkAAIDIMLkEAABAZGJ7zqXjpfvraPdrd+2WfNtduyXaHdtduyXPdo5cAgAAIDJMLgEAABAZJpcAAACITGzPuayVdaEk3zWxJN92127Jt921W6Ldsd21W/JtZ51LAAAA7InQk8unT5/q0qVLSqfTSiQSmpqaKtkeBIGGhobU3NysQ4cOqaenR69evYpqvAAAAIix0JPL1dVVnT59WuPj4xW3P3jwQA8fPtTExITm5+d1+PBhXbx4UR8/fvzqwQIAACDeQp9z2dvbq97e3orbgiDQ2NiY7t69q8uXL0uSHj9+rFQqpampKV27dm3H7+O4LtQ62v3aXbsl33bXbol2x3bXbsmzPdILehYXF5XNZtXT01N4rrGxUV1dXZqbm6s4uczn88rn84XHKysrUQ4p1mj3a3ftlnzbXbsl2h3bXbsl7/ZykV7Qk81mJUmpVKrk+VQqVdhWbnR0VI2NjYV/ra2tUQ4p1mj3a3ftlnzbXbsl2h3bXbsl7/ZyiaD8+GeYnRMJPXnyRFeuXJEkzc7O6ttvv9XS0pKam5sLr/vuu++USCQ0OTm54XdUmum3trbW5GHk4iaJdod2127Jt921W6Ldsd21W/JtL++uJNKvxZuamiRJuVyuZHKZy+XU0dFRcZ9kMqlkMrnh+VpZF0rafG0o2mu33bVb8m137ZZod2x37ZZ82/dsncv29nY1NTVpZmam8NzKyorm5+fV3d0d5VsBAAAghkIfufzw4YNev35deLy4uKiXL1/q6NGjamtr0+3bt3X//n2dPHlS7e3tunfvntLpdOGrcwAAANSu0JPL58+f68KFC4XHAwMDkqS+vj49evRIg4ODWl1d1c2bN/Xu3TudO3dO09PTqq+vD/U+tXaOQhi0+7W7dku+7a7dEu2O7a7dkmd76Mnl+fPnN7xx+SBGRkY0MjISejAAAADY37i3OAAAACLD5BIAAACRiXQpoijVyqX7UrjL9yXadzKGuLe7dku+7a7dEu2O7a7dkm/7ni1FBAAAAG9MLgEAABCZ2H0tXulK9K1u/r7djeGrtW+Y373+81ZX2W+2nfbqjq9av9u1u/hnt3bX7uKfad9crbW7dhf/7Na+0+71F8VKJpMJJNXkv0wmQ7tZu2u3c7trN+2e7a7dzu3bdQdBECSCYCdT0N2ztrampaUlBUGgtrY2ZTKZbW+QvpvWb0QfZlxBEOj9+/dKp9Oqq9v8TIQ4t39JtxSufWFhQadOnYpVt+T7mUu+7a7dkm87f+P8PnPJt72a3VIMvxavq6vT8ePHC4dfjxw5EpsPo1jYce3kCqv90P4lY9ppe0tLyxe/x25w/cwl33bXbsm3nb9xfp+55NtejW6JC3oAAAAQISaXAAAAiExsJ5fJZFLDw8NKJpN7PZQSuzGuOLa7dku0O7a7dku+7a7dEu2O7dUeU+wu6AEAAMD+FdsjlwAAANh/qja5HB8f14kTJ1RfX6+uri49e/asWm8FAACAmKjK1+KTk5O6fv26JiYm1NXVpbGxMf3www9aWFjQN998s+W+6+tCNTQ0bLh5+n4Vdk0s2vd/u2u35Nvu2i3R7tju2i35todZ57Iqd+jp7OwMbt26VXj8+fPnIJ1OB6Ojo9vuW6sr2st4NX/ndtdu53bXbto92127ndt3coeeyBdR//Tpk168eKHvv/++8FxdXZ16eno0Nze34fX5fF75fL7wOPjbgdTiVePLF+1cXl4u/LzVtmruW759u30lqaGhoeQx7bXf7tot+ba7dku0F3Npd+2WfNvLuyvadvoZ0ps3bwJJwezsbMnzd+7cCTo7Oze8fnh4uOLMeHl5ufCa8m3FttpWzX3Lt2+3b3kT7R7trt3O7a7dtHu2u3Y7t5d3VxL5OZdLS0tqaWnR7Oysuru7C88PDg7qxx9/1Pz8fMnry2f66/e7LFY+xOJzF7baVs19y7dvtW1lZUWNjY1aXl4uuc0S7bXb7tot+ba7dku0O7a7dku+7Zt1VxL51+LHjh3TgQMHlMvlSp7P5XJqamra8PpkMhmrhUV3E+1+7a7dkm+7a7dEu2O7a7fk3V4u8qWIDh48qDNnzmhmZqbw3NrammZmZkqOZAIAAKD2RH7kUpIGBgbU19ens2fPqrOzU2NjY1pdXdWNGzeq8XYAAACIiapMLq9evaq3b99qaGhI2WxWHR0dmp6eViqV2vHvKP5O/0vPi6jmvuXbt9s3DNr92l27Jd92126Jdsd2127Js70qk0tJ6u/vV39/f7V+PQAAAGKIe4sDAAAgMlU7cvm1ihft3M+X7kt/v3x/p2jffgxxb3ftlnzbXbsl2h3bXbsl3/Yw3Ry5BAAAQGSYXAIAACAyTC4BAAAQmdiec+l46f462v3aXbsl33bXbol2x3bXbsmznSOXAAAAiAyTSwAAAESGySUAAAAiE9tzLmtlXSjJd00sybfdtVvybXftlmh3bHftlnzbWecSAAAAe4LJJQAAACLD5BIAAACRie05l47rQq2j3a/dtVvybXftlmh3bHftljzbOXIJAACAyDC5BAAAQGRi+7V4rVy6L/kuWyD5trt2S77trt0S7Y7trt2SbztLEQEAAGBPMLkEAABAZJhcAgAAIDKxPefS8dL9dbT7tbt2S77trt0S7Y7trt2SZztHLgEAABAZJpcAAACIDJNLAAAARCa251zWyrpQku+aWJJvu2u35Nvu2i3R7tju2i35trPOJQAAAPZE6Mnl06dPdenSJaXTaSUSCU1NTZVsD4JAQ0NDam5u1qFDh9TT06NXr15FNV4AAADEWOjJ5erqqk6fPq3x8fGK2x88eKCHDx9qYmJC8/PzOnz4sC5evKiPHz9+9WABAAAQb6HPuezt7VVvb2/FbUEQaGxsTHfv3tXly5clSY8fP1YqldLU1JSuXbu24/dxXBdqHe1+7a7dkm+7a7dEu2O7a7fk2R7pBT2Li4vKZrPq6ekpPNfY2Kiuri7Nzc1VnFzm83nl8/nC45WVlSiHFGu0+7W7dku+7a7dEu2O7a7dknd7uUgv6Mlms5KkVCpV8nwqlSpsKzc6OqrGxsbCv9bW1iiHFGu0+7W7dku+7a7dEu2O7a7dknd7uURQfvwzzM6JhJ48eaIrV65IkmZnZ/Xtt99qaWlJzc3Nhdd99913SiQSmpyc3PA7Ks30W1tba/IwcnGTRLtDu2u35Nvu2i3R7tju2i35tpd3VxLp1+JNTU2SpFwuVzK5zOVy6ujoqLhPMplUMpnc8HytrAslbb42FO212+7aLfm2u3ZLtDu2u3ZLvu17ts5le3u7mpqaNDMzU3huZWVF8/Pz6u7ujvKtAAAAEEOhj1x++PBBr1+/LjxeXFzUy5cvdfToUbW1ten27du6f/++Tp48qfb2dt27d0/pdLrw1TkAAABqV+jJ5fPnz3XhwoXC44GBAUlSX1+fHj16pMHBQa2ururmzZt69+6dzp07p+npadXX14d6n1o7RyEM2v3aXbsl33bXbol2x3bXbsmzPfTk8vz58xveuHwQIyMjGhkZCT0YAAAA7G/cWxwAAACRYXIJAACAyES6FFGUauXSfSnc5fsS7TsZQ9zbXbsl33bXbol2x3bXbsm3fc+WIgIAAIA3JpcAAACITOy+Fq90JfpWN3/f7sbw1do3zO9e/3mrq+w32057dcdXrd/t2l38s1u7a3fxz7RvrtbaXbuLf3Zr32n3+otiJZPJBJJq8l8mk6HdrN2127ndtZt2z3bXbuf27bqDIAgSQbCTKejuWVtb09LSkoIgUFtbmzKZzLY3SN9N6zeiDzOuIAj0/v17pdNp1dVtfiZCnNu/pFsK176wsKBTp07Fqlvy/cwl33bXbsm3nb9xfp+55NtezW4phl+L19XV6fjx44XDr0eOHInNh1Es7Lh2coXVfmj/kjHttL2lpeWL32M3uH7mkm+7a7fk287fOL/PXPJtr0a3xAU9AAAAiBCTSwAAAEQmtpPLZDKp4eFhJZPJvR5Kid0YVxzbXbsl2h3bXbsl33bXbol2x/Zqjyl2F/QAAABg/4rtkUsAAADsP1WbXI6Pj+vEiROqr69XV1eXnj17Vq23AgAAQExU5WvxyclJXb9+XRMTE+rq6tLY2Jh++OEHLSws6Jtvvtly3/V1oRoaGjbcPH2/CrsmFu37v921W/Jtd+2WaHdsd+2WfNvDrHNZlTv0dHZ2Brdu3So8/vz5c5BOp4PR0dFt963VFe1lvJq/c7trt3O7azftnu2u3c7tO7lDT+SLqH/69EkvXrzQ999/X3iurq5OPT09mpub2/D6fD6vfD5feBz87UBq8arx5Yt2Li8vF37eals19y3fvt2+ktTQ0FDymPbab3ftlnzbXbsl2ou5tLt2S77t5d0VbTv9DOnNmzeBpGB2drbk+Tt37gSdnZ0bXj88PFxxZry8vFx4Tfm2Ylttq+a+5du327e8iXaPdtdu53bXbto92127ndvLuyuJ/JzLpaUltbS0aHZ2Vt3d3YXnBwcH9eOPP2p+fr7k9eUz/fX7XS4vLxdm+uXnKhQPeatt1dy3fPt2+0oqaZJod2h37ZZ82127Jdod2127Jd/28u5KIv9a/NixYzpw4IByuVzJ87lcTk1NTRten0wmKy7iWXwY9mv+E6q1b/n2rbatrKxUPKROe+22u3ZLvu2u3RLtju2u3ZJv+2bdlUS+FNHBgwd15swZzczMFJ5bW1vTzMxMyZFMAAAA1J7Ij1xK0sDAgPr6+nT27Fl1dnZqbGxMq6urunHjRjXeDgAAADFRlcnl1atX9fbtWw0NDSmbzaqjo0PT09NKpVLVeDsAAADERFUml5LU39+v/v7+L96/1k6ADYN2v3bXbsm33bVbot2x3bVb8mzn3uIAAACIDJNLAAAARKZqX4t/rVq5dF8Kd/m+RPtOxhD3dtduybfdtVui3bHdtVvybd/TpYgAAADgi8klAAAAIsPkEgAAAJGJ7TmXjpfur6Pdr921W/Jtd+2WaHdsd+2WPNs5cgkAAIDIMLkEAABAZJhcAgAAIDKxPeeyVtaFknzXxJJ82127Jd92126Jdsd2127Jt511LgEAALAnmFwCAAAgMkwuAQAAEJnYnnPpuC7UOtr92l27Jd92126Jdsd2127Js50jlwAAAIgMk0sAAABEJrZfi9fKpfuS77IFkm+7a7fk2+7aLdHu2O7aLfm2sxQRAAAA9gSTSwAAAESGySUAAAAiE9tzLh0v3V9Hu1+7a7fk2+7aLdHu2O7aLXm2c+QSAAAAkWFyCQAAgMgwuQQAAEBkYnvOZa2sCyX5rokl+ba7dku+7a7dEu2O7a7dkm8761wCAABgT4SeXD59+lSXLl1SOp1WIpHQ1NRUyfYgCDQ0NKTm5mYdOnRIPT09evXqVVTjBQAAQIyFnlyurq7q9OnTGh8fr7j9wYMHevjwoSYmJjQ/P6/Dhw/r4sWL+vjx41cPFgAAAPEW+pzL3t5e9fb2VtwWBIHGxsZ09+5dXb58WZL0+PFjpVIpTU1N6dq1azt+H8d1odbR7tfu2i35trt2S7Q7trt2S57tkV7Qs7i4qGw2q56ensJzjY2N6urq0tzcXMXJZT6fVz6fLzxeWVmJckixRrtfu2u35Nvu2i3R7tju2i15t5eL9IKebDYrSUqlUiXPp1KpwrZyo6OjamxsLPxrbW2NckixRrtfu2u35Nvu2i3R7tju2i15t5dLBOXHP8PsnEjoyZMnunLliiRpdnZW3377rZaWltTc3Fx43XfffadEIqHJyckNv6PSTL+1tbUmDyMXN0m0O7S7dku+7a7dEu2O7a7dkm97eXclkX4t3tTUJEnK5XIlk8tcLqeOjo6K+ySTSSWTyQ3P18q6UNLma0PRXrvtrt2Sb7trt0S7Y7trt+TbvmfrXLa3t6upqUkzMzOF51ZWVjQ/P6/u7u4o3woAAAAxFPrI5YcPH/T69evC48XFRb18+VJHjx5VW1ubbt++rfv37+vkyZNqb2/XvXv3lE6nC1+dAwAAoHaFnlw+f/5cFy5cKDweGBiQJPX19enRo0caHBzU6uqqbt68qXfv3uncuXOanp5WfX19qPeptXMUwqDdr921W/Jtd+2WaHdsd+2WPNtDTy7Pnz+/4Y3LBzEyMqKRkZHQgwEAAMD+xr3FAQAAEBkmlwAAAIhMpEsRRalWLt2Xwl2+L9G+kzHEvd21W/Jtd+2WaHdsd+2WfNv3bCkiAAAAeGNyCQAAgMjE7mvxSleib3Xz9+1uDF+tfcP87vWft7rKfrPttFd3fNX63a7dxT+7tbt2F/9M++Zqrd21u/hnt/addq+/KFYymUwgqSb/ZTIZ2s3aXbud2127afdsd+12bt+uOwiCIBEEO5mC7p61tTUtLS0pCAK1tbUpk8lse4P03bR+I/ow4wqCQO/fv1c6nVZd3eZnIsS5/Uu6pXDtCwsLOnXqVKy6Jd/PXPJtd+2WfNv5G+f3mUu+7dXslmL4tXhdXZ2OHz9eOPx65MiR2HwYxcKOaydXWO2H9i8Z007bW1pavvg9doPrZy75trt2S77t/I3z+8wl3/ZqdEtc0AMAAIAIMbkEAABAZGI7uUwmkxoeHlYymdzroZTYjXHFsd21W6Ldsd21W/Jtd+2WaHdsr/aYYndBDwAAAPav2B65BAAAwP5Ttcnl+Pi4Tpw4ofr6enV1denZs2fVeisAAADERFW+Fp+cnNT169c1MTGhrq4ujY2N6YcfftDCwoK++eabLfddXxeqoaFhw83T96uwa2LRvv/bXbsl33bXbol2x3bXbsm3Pcw6l1W5Q09nZ2dw69atwuPPnz8H6XQ6GB0d3XbfWl3RXsar+Tu3u3Y7t7t20+7Z7trt3L6TO/REvoj6p0+f9OLFC33//feF5+rq6tTT06O5ubkNr8/n88rn84XHwd8OpBavGl++aOfy8nLh5622VXPf8u3b7StJDQ0NJY9pr/12127Jt921W6K9mEu7a7fk217eXdG208+Q3rx5E0gKZmdnS56/c+dO0NnZueH1w8PDFWfGy8vLhdeUbyu21bZq7lu+fbt9y5to92h37XZud+2m3bPdtdu5vby7ksjPuVxaWlJLS4tmZ2fV3d1deH5wcFA//vij5ufnS15fPtNfv9/l8vJyYaZffq5C8ZC32lbNfcu3b7evpJImiXaHdtduybfdtVui3bHdtVvybS/vriTyr8WPHTumAwcOKJfLlTyfy+XU1NS04fXJZLLiIp7Fh2G/5j+hWvuWb99q28rKSsVD6rTXbrtrt+Tb7tot0e7Y7tot+bZv1l1J5EsRHTx4UGfOnNHMzEzhubW1Nc3MzJQcyQQAAEDtifzIpSQNDAyor69PZ8+eVWdnp8bGxrS6uqobN25U4+0AAAAQE1WZXF69elVv377V0NCQstmsOjo6ND09rVQqVY23AwAAQExUZXIpSf39/erv7//i/WvtBNgwaPdrd+2WfNtduyXaHdtduyXPdu4tDgAAgMgwuQQAAEBkqva1+NeqlUv3pXCX70u072QMcW937ZZ82127Jdod2127Jd/2PV2KCAAAAL6YXAIAACAyTC4BAAAQmdiec+l46f462v3aXbsl33bXbol2x3bXbsmznSOXAAAAiAyTSwAAAESGySUAAAAiE9tzLmtlXSjJd00sybfdtVvybXftlmh3bHftlnzbWecSAAAAe4LJJQAAACLD5BIAAACRie05l47rQq2j3a/dtVvybXftlmh3bHftljzbOXIJAACAyDC5BAAAQGRi+7V4rVy6L/kuWyD5trt2S77trt0S7Y7trt2SbztLEQEAAGBPMLkEAABAZJhcAgAAIDKxPefS8dL9dbT7tbt2S77trt0S7Y7trt2SZztHLgEAABAZJpcAAACIDJNLAAAARCa251zWyrpQku+aWJJvu2u35Nvu2i3R7tju2i35tld1ncunT5/q0qVLSqfTSiQSmpqa2jCIoaEhNTc369ChQ+rp6dGrV6/Cvg0AAAD2odCTy9XVVZ0+fVrj4+MVtz948EAPHz7UxMSE5ufndfjwYV28eFEfP3786sECAAAg3kJ/Ld7b26ve3t6K24Ig0NjYmO7evavLly9Lkh4/fqxUKqWpqSldu3bt60YLAACAWIv0nMvFxUVls1n19PQUnmtsbFRXV5fm5uZCTS4d14VaR7tfu2u35Nvu2i3R7tju2i15tkc6ucxms5KkVCpV8nwqlSpsK5fP55XP5wuPV1ZWohxSrNHu1+7aLfm2u3ZLtDu2u3ZL3u3l9nwpotHRUTU2Nhb+tba27vWQdg3tfu2u3ZJvu2u3RLtju2u35N1eLhGUH/8Ms3MioSdPnujKlSuSpD/96U/6+c9/rp9++kkdHR2F1/3yl79UR0eHfvOb32z4HZVm+q2trTV5GLm4SaLdod21W/Jtd+2WaHdsd+2WfNvLuyuJ9Gvx9vZ2NTU1aWZmpjC5XFlZ0fz8vP7pn/6p4j7JZFLJZHLD87WyLpS0+dpQtNduu2u35Nvu2i3R7tju2i35todZ5zL05PLDhw96/fp14fHi4qJevnypo0ePqq2tTbdv39b9+/d18uRJtbe36969e0qn04WjmwAAAKhdoSeXz58/14ULFwqPBwYGJEl9fX169OiRBgcHtbq6qps3b+rdu3c6d+6cpqenVV9fH92oAQAAEEuhJ5fnz5/fcHi1WCKR0MjIiEZGRr5qYLV2jkIYtPu1u3ZLvu2u3RLtju2u3ZJn+55fLQ4AAIDaweQSAAAAkWFyCQAAgMhEuhRRlGrl0n0p3OX7Eu07GUPc2127Jd92126Jdsd2127Jtz1MN0cuAQAAEBkmlwAAAIhM7L4WLz90K2198/ftbgxfrX3D/O71nyu1FaP9y97ja8ZXrd/t2l38s1u7a3fxz7RvrtbaXbuLf3Zr32n3+otiJZPJBJJq8l8mk6HdrN2127ndtZt2z3bXbuf27bqDIAgSQbCTKejuWVtb09LSkoIgUFtbmzKZzLY3SN9N6zeiDzOuIAj0/v17pdNp1dVtfiZCnNu/pFsK176wsKBTp07Fqlvy/cwl33bXbsm3nb9xfp+55NtezW4phl+L19XV6fjx44XDr0eOHInNh1Es7Lh2coXVfmj/kjHttL2lpeWL32M3uH7mkm+7a7fk287fOL/PXPJtr0a3xAU9AAAAiBCTSwAAAEQmtpPLZDKp4eFhJZPJvR5Kid0YVxzbXbsl2h3bXbsl33bXbol2x/Zqjyl2F/QAAABg/6rakcvx8XGdOHFC9fX16urq0rNnz6r1VgAAAIiJqkwuJycnNTAwoOHhYf3hD3/Q6dOndfHiRf35z3+uxtsBAAAgJqrytXhXV5d+8Ytf6Le//a2kv6711Nraqn/+53/Wr3/96y33XV8XqqGhYcPN0/ersGti0b7/2127Jd92126Jdsd2127Jtz3MOpeR36Enn88HBw4cCJ48eVLy/PXr14Nf/epX2+5fqyvay3g1f+d2127ndtdu2j3bXbud23dyh57IF1H/y1/+os+fPyuVSpU8n0ql9Mc//nHD6/P5vPL5fOFx8LcDqcWrxpcv2rm8vFz4eatt1dy3fPt2+0pSQ0NDyWPaa7/dtVvybXftlmgv5tLu2i35tpd3V7Tt9DOkN2/eBJKC2dnZkufv3LkTdHZ2bnj98PBwxZnx8vJy4TXl24ptta2a+5Zv327f8ibaPdpdu53bXbtp92x37XZuL++uJPJzLj99+qSf/exn+t3vfqcrV64Unu/r69O7d+/0+9//vuT15TP99ftdFisfYvG5C1ttq+a+5du32raysqLGxkYtLy+X3GaJ9tptd+2WfNtduyXaHdtduyXf9s26K4n8a/GDBw/qzJkzmpmZKUwu19bWNDMzo/7+/g2vTyaTsVpYdDfR7tfu2i35trt2S7Q7trt2S97t5SKfXErSwMCA+vr6dPbsWXV2dmpsbEyrq6u6ceNGNd4OAAAAMVGVyeXVq1f19u1bDQ0NKZvNqqOjQ9PT0xsu8gEAAEBtqcrkUpL6+/srfg2+U8Xf6X/peRHV3Ld8+3b7hkG7X7trt+Tb7tot0e7Y7totebZX7faPAAAA8MPkEgAAAJGp2tfiX6t40c79fOm+9PfL93eK9u3HEPd2127Jt921W6Ldsd21W/JtD9PNkUsAAABEhsklAAAAIsPkEgAAAJGJ7TmXjpfur6Pdr921W/Jtd+2WaHdsd+2WPNs5cgkAAIDIMLkEAABAZJhcAgAAIDKxPeeyVtaFknzXxJJ82127Jd92126Jdsd2127Jt511LgEAALAnmFwCAAAgMkwuAQAAEJnYnnPpuC7UOtr92l27Jd92126Jdsd2127Js50jlwAAAIgMk0sAAABEJrZfi9fKpfuS77IFkm+7a7fk2+7aLdHu2O7aLfm2sxQRAAAA9gSTSwAAAESGySUAAAAiE9tzLh0v3V9Hu1+7a7fk2+7aLdHu2O7aLXm2c+QSAAAAkWFyCQAAgMgwuQQAAEBkYnvOZa2sCyX5rokl+ba7dku+7a7dEu2O7a7dkm8761wCAABgT4SeXD59+lSXLl1SOp1WIpHQ1NRUyfYgCDQ0NKTm5mYdOnRIPT09evXqVVTjBQAAQIyFnlyurq7q9OnTGh8fr7j9wYMHevjwoSYmJjQ/P6/Dhw/r4sWL+vjx41cPFgAAAPEW+pzL3t5e9fb2VtwWBIHGxsZ09+5dXb58WZL0+PFjpVIpTU1N6dq1azt+H8d1odbR7tfu2i35trt2S7Q7trt2S57tkV7Qs7i4qGw2q56ensJzjY2N6urq0tzcXMXJZT6fVz6fLzxeWVmJckixRrtfu2u35Nvu2i3R7tju2i15t5eL9IKebDYrSUqlUiXPp1KpwrZyo6OjamxsLPxrbW2NckixRrtfu2u35Nvu2i3R7tju2i15t5dLBOXHP8PsnEjoyZMnunLliiRpdnZW3377rZaWltTc3Fx43XfffadEIqHJyckNv6PSTL+1tbUmDyMXN0m0O7S7dku+7a7dEu2O7a7dkm97eXclkX4t3tTUJEnK5XIlk8tcLqeOjo6K+ySTSSWTyQ3P18q6UNLma0PRXrvtrt2Sb7trt0S7Y7trt+TbvmfrXLa3t6upqUkzMzOF51ZWVjQ/P6/u7u4o3woAAAAxFPrI5YcPH/T69evC48XFRb18+VJHjx5VW1ubbt++rfv37+vkyZNqb2/XvXv3lE6nC1+dAwAAoHaFnlw+f/5cFy5cKDweGBiQJPX19enRo0caHBzU6uqqbt68qXfv3uncuXOanp5WfX19qPeptXMUwqDdr921W/Jtd+2WaHdsd+2WPNtDTy7Pnz+/4Y3LBzEyMqKRkZHQgwEAAMD+xr3FAQAAEBkmlwAAAIhMpEsRRalWLt2Xwl2+L9G+kzHEvd21W/Jtd+2WaHdsd+2WfNv3bCkiAAAAeGNyCQAAgMjE7mvxSleib3Xz9+1uDF+tfcP87vWft7rKfrPttFd3fNX63a7dxT+7tbt2F/9M++Zqrd21u/hnt/addq+/KFYymUwgqSb/ZTIZ2s3aXbud2127afdsd+12bt+uOwiCIBEEO5mC7p61tTUtLS0pCAK1tbUpk8lse4P03bR+I/ow4wqCQO/fv1c6nVZd3eZnIsS5/Uu6pXDtCwsLOnXqVKy6Jd/PXPJtd+2WfNv5G+f3mUu+7dXslmL4tXhdXZ2OHz9eOPx65MiR2HwYxcKOaydXWO2H9i8Z007bW1pavvg9doPrZy75trt2S77t/I3z+8wl3/ZqdEtc0AMAAIAIMbkEAABAZGI7uUwmkxoeHlYymdzroZTYjXHFsd21W6Ldsd21W/Jtd+2WaHdsr/aYYndBDwAAAPav2B65BAAAwP7D5BIAAACRYXIJAACAyDC5BAAAQGRiO7kcHx/XiRMnVF9fr66uLj179mxX3//p06e6dOmS0um0EomEpqamSrYHQaChoSE1Nzfr0KFD6unp0atXr776fV27Jdr3qt21W6Kdv3F85sVqtd21W9q79lhOLicnJzUwMKDh4WH94Q9/0OnTp3Xx4kX9+c9/3rUxrK6u6vTp0xofH6+4/cGDB3r48KEmJiY0Pz+vw4cP6+LFi/r48eMXv6drt0T7Xra7dku08zduIz7z2mt37Zb2rl3b3n18D3R2dga3bt0qPP78+XOQTqeD0dHRPRmPpODJkyeFx2tra0FTU1Pwr//6r4Xn3r17FySTyeA//uM/vvh9XLuDgPa4tLt2BwHt/I3jM3dod+0Ogt1tj92Ry0+fPunFixfq6ekpPFdXV6eenh7Nzc3t4cj+bnFxUdlstmSMjY2N6urq+uIxunZLtMe53bVbop2/cX/FZ17b7a7dUvXapRh+Lf6Xv/xFnz9/ViqVKnk+lUopm83u0ahKrY8jyjG6dku0x7ndtVuiXeJv3Do+89ptd+2WqtcuxXByCQAAgP0rdpPLY8eO6cCBA8rlciXP53I5NTU17dGoSq2PI8oxunZLtMe53bVbol3ib9w6PvPabXftlqrXLsVwcnnw4EGdOXNGMzMzhefW1tY0MzOj7u7uPRzZ37W3t6upqalkjCsrK5qfn//iMbp2S7THud21W6Kdv3F/xWde2+2u3VL12iXF82rx//zP/wySyWTw6NGj4H//93+DmzdvBv/wD/8QZLPZXRvD+/fvg59++in46aefAknBv/3bvwU//fRT8P/+3/8LgiAI/uVf/iX4h3/4h+D3v/998D//8z/B5cuXg/b29uD//u//vvg9XbuDgPa9bHftDgLa+RvHZ+7Q7todBHvXHsvJZRAEwb//+78HbW1twcGDB4POzs7gv//7v3f1/f/rv/4rkLThX19fXxAEf72E/969e0EqlQqSyWTwj//4j8HCwsJXv69rdxDQvlftrt1BQDt/4/jMHdpdu4Ng79oTQRAEX3fsEwAAAPir2J1zCQAAgP2LySUAAAAiw+QSAAAAkWFyCQAAgMgwuQQAAEBkmFwCAAAgMkwuAQAAEBkmlwAAAIgMk0sAAABEhsklAAAAIsPkEgAAAJFhcgkAAIDI/P/+OxySwi9yIwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x800 with 200 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_samples(model, num_steps=8, temp=1.0, top_p=0.0, top_k=0.0, n_samples=10):\n",
    "    fig = plt.figure(figsize=(8., 8.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(10, n_samples), axes_pad=0.1)\n",
    "    for label in range(10):\n",
    "        for sample_idx in range(n_samples):\n",
    "            grid_idx = label * n_samples + sample_idx\n",
    "            \n",
    "            seq = torch.zeros(50, dtype=torch.long, device=device)\n",
    "            mask = torch.ones(50, dtype=torch.bool, device=device)\n",
    "            seq[0] = label\n",
    "            mask[0] = False\n",
    "            output = model.generate(seq, mask, num_steps=num_steps, temp=temp, top_p=top_p, top_k=top_k, return_history=False)\n",
    "            \n",
    "            reconst = detokenize_MNIST(output, patch_size=2, account_for_labels=True).cpu()\n",
    "            grid[grid_idx].imshow(reconst[0], cmap='Greys', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "    \n",
    "generate_samples(model, num_steps=8, temp=0.7, top_p=0.9, top_k=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c77608-5cb3-4f7e-9ffc-fd36aef83cdb",
   "metadata": {},
   "source": [
    "### 2.5 Open-ended questions (10 points each)\n",
    "\n",
    "Please answer the following questions. You may use additional cells to demonstrate your answers if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ae3d6-ddaa-44d9-b5d8-9748a2ec3b5f",
   "metadata": {},
   "source": [
    "#### 2.5.1 Intermediate generation steps\n",
    "\n",
    "`model.generate` has an optional flag `return_history`. Show the intermediate generation steps. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5015471-fa5b-41bc-a190-ed5930e2d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb60ba-521e-43cd-9c01-c94507740fa4",
   "metadata": {},
   "source": [
    "#### 2.5.2 Number of inference steps\n",
    "\n",
    "With MaskGIT, we can freely choose the number of inference steps `k`. We default to `k=8`, but how does generation with `k` = 1, 4, 8, 16, 32, 49 perform? What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba08d6c1-4570-4d92-ade9-c59a4e8fc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6959e3e-6682-4b9e-afd4-d20362086f58",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 3 Training nanoGPT on TinyStories\n",
    "\n",
    "Masked generation is quite common for image generation, but has seen a recent resurgence for language models too (e.g. see [LLaDA](https://ml-gsai.github.io/LLaDA-demo/)). \n",
    "Let's run a little experiment and train a masked model on TinyStories, just as we did with nanoGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f284f-4ffb-4611-98c1-302b09cd516a",
   "metadata": {},
   "source": [
    "### 3.1 Loading the tokenizer and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9aa2c0e-71af-4d51-9716-9ab4ca17f459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[SOS]', 'eos_token': '[EOS]', 'unk_token': '<|endoftext|>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50257: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50258: AddedToken(\"[SOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50259: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", trust_remote_code=True)\n",
    "\n",
    "# Add padding, start-of-sequence, and end-of-sequence tokens\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.add_special_tokens({\n",
    "    'bos_token': '[SOS]',\n",
    "    'eos_token': '[EOS]',\n",
    "})\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[SOS] $A [EOS]\",\n",
    "    special_tokens=[('[EOS]', tokenizer.eos_token_id), ('[SOS]', tokenizer.bos_token_id)],\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08407185-c1f8-48fe-9bc8-5b0d35c33e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, text_tokenizer):\n",
    "    \"\"\" Helper function to turn token sequences back to well-formatted text. \"\"\"\n",
    "    decoded = text_tokenizer.decode(token_ids)\n",
    "    # Remove [SOS], [EOS], and [PAD] tokens along with surrounding horizontal whitespace only.\n",
    "    decoded = re.sub(r'[ \\t]*\\[(SOS|EOS|PAD)\\][ \\t]*', ' ', decoded)\n",
    "    # Collapse extra horizontal spaces in each line without touching newline characters.\n",
    "    decoded = '\\n'.join([re.sub(r'[ \\t]+', ' ', line).strip() for line in decoded.splitlines()])\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e1ee7-7805-42ca-a5f7-7a9f45dadb9e",
   "metadata": {},
   "source": [
    "### 3.2 Training the model\n",
    "\n",
    "We defined a training config for you in: `cfgs/nanoMaskGIT/tinystories_d8w512.yaml`. Please familiarize yourself with all parts.\n",
    "Please don't forget to replace the Weights & Bias entity with your own.\n",
    "\n",
    "On a 2xV100 node, you can start the training like:\n",
    "```\n",
    "OMP_NUM_THREADS=1 torchrun --nproc_per_node=2 run_training.py --config cfgs/nanoMaskGIT/tinystories_d8w512.yaml\n",
    "```\n",
    "\n",
    "This training should take over one hour. You should reach a final validation loss around 2.05, and your loss curves should look something like the following:\n",
    "\n",
    "<img src=\"./assets/nanoMaskGIT_tinystories.png\" alt=\"nanoMaskGIT TinyStories loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148e772-641d-401f-8759-d470913c07a8",
   "metadata": {},
   "source": [
    "### 3.3 Show your loss curves (10 points)\n",
    "\n",
    "Screenshot your loss curves and show them here. Add the image to the `assets` directory and change the path in the markdown. You will get 10 points for reasonable loss curves (similar to the sample loss curves above).\n",
    "\n",
    "<img src=\"./assets/your_loss_curves.png\" alt=\"nanoMaskGIT TinyStories loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae662ed8-4d89-419f-9419-72dfe1539a8b",
   "metadata": {},
   "source": [
    "### 3.4 Evaluating the model (10 points)\n",
    "\n",
    "After you completed the training, load the model with the following cell. You may need to adjust the path if you changed it.\n",
    "You will get 10 points if the outputs look reasonable (similar to the sample outputs provided below).\n",
    "\n",
    "Hint: You can also load intermediate safetensors checkpoints to check the progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "461ee865-a0a8-47d1-ac34-577a16c77fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.174528M parameters\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = './outputs/nanoMaskGIT/tinystories_d8w512/checkpoint-final.safetensors'\n",
    "model = load_model_from_safetensors(ckpt_path, device=device)\n",
    "print(f'{model.get_num_params() / 10**6}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b76935-50bd-4467-b72d-30955eb516b7",
   "metadata": {},
   "source": [
    "Let's generate some random (unconditional) stories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af25761c-ae44-4e67-8c77-505d6be35a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom day, a boy named Lily and Tom mom went to the gym with their mom, the. The saw how big dogs had a, red dog. Lily came Tom. Tom said a police had to police car who lived. They all liked to be nice and friendly.\n",
      "\n",
      "Tom, Lily wanted hello to the kids kids, said they. The said they wanted to be okay for the one and hurt someone They wanted to be like the. Lily all made fun and They asked him to they wanted, so the kids lady agreed yes.\n",
      "\n",
      "After they left, Tom and Lily went a visit class. They had the law go to wait back the day. Tom said about had an emergency and asked Lily to the too. Lily said it was the fun at school.\n",
      "\n",
      "Tom brought and said thank you to the great dog in the gym. the kids were proud of fun. all went friends and learned many talk about big dogs. .\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Once sun went a outside, and it one special was happy outside!\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "L and and are twins to They like to play in the parkadow. They have many trees that hold from the wind. They like the leaves colors and made a path for their parents.\n",
      "\n",
      "One day, they see a big green structure. It is very tall and soft and has a ladder.. Lily think it is a treasure game. They follow it is a far distance.\n",
      "go past the factory and start a way. They climb to the factory of the house. They go and see and wait about the park. They\n",
      "\n",
      "\" is coming from there?\" Lily calls.\n",
      "\n",
      "But mom and dad hear his name, he is the man, the Tom. them hear the name of the factory. He says \"He, the. Do is looking for food?\"\n",
      "Lily and Tom say \" in there \"Fire! How do you know? it will so nice!\" They\n",
      "The run to the factory with smoke and man puts some fire on his coat. He puts the fireometer to They make y fire..\n",
      "put them fire in a place. Lily are proud.\n",
      "say: to Tom and dad, \", dad is what nice!\"\n",
      "\n",
      "They go their mom to put fire and pretend to. They tell his about their orange\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "One upon a time, some thief.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Once upon a time there there was a little girl called Tim. She was a very strong and she loved to play all day. One day, she went to the park with her friends. She played with the swings and the slide. the sunshine was very bright!\n",
      "\n",
      "But then, Tim, was, didn her play with her, but wanted to let him. Tim showed off her favorite shirt to keep him in his pocket. Lily was sad and said that she had not worn one.\n",
      "\n",
      "Tim took Lily outside, and hugged her, toldThat Lily was mean, wouldmy wear his own shirt.\" Lily felt bad, wanted him to hug, so she asked Tim to borrow his shirt. Lily looked it and her head smiled. Mia was crying and She saw a cute with pretty friend, its color shirt. Tim shared her shirt and helped her smile his head. The boy felt happy too and, \"That you're my very friend?\" He was left feeling sad, but he had a smile on his face, knowing that his friend would be safe\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    seq = torch.zeros(256, dtype=torch.long, device=device)\n",
    "    mask = torch.ones(256, dtype=torch.bool, device=device)\n",
    "    output = model.generate(seq, mask, num_steps=128, temp=1.0, top_k=100, return_history=False)\n",
    "    print(token_ids_to_text(output[0], text_tokenizer=tokenizer))\n",
    "    print('\\n' + '-'*50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb8ab8-0a6b-446c-af41-acacf2c584cd",
   "metadata": {},
   "source": [
    "### 3.5 Open-ended questions (10 points each)\n",
    "\n",
    "Please answer the following questions. You may use additional cells to demonstrate your answers if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c1f71-4a23-4eb9-8f06-2854fc66b480",
   "metadata": {},
   "source": [
    "#### 3.5.1 Intermediate generation steps\n",
    "\n",
    "Similar as in 2.5.1, `model.generate` has an optional flag `return_history`. Show the intermediate generation steps. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d5a974f-773a-4851-a3d4-f3ded2a45022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee85f348-ce6a-4cf7-8ba8-ae5ba0cacf6f",
   "metadata": {},
   "source": [
    "#### 3.5.2 Number of inference steps\n",
    "\n",
    "Similar as in 2.5.1, with MaskGIT, we can freely choose the number of inference steps `k`. We default to `k=8`, but how does generation with `k` = 1, 4, 8, 16, 32, 49 perform? What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "743cad67-9e6e-47fd-8a8a-e43161548927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a6b31c-2d8b-41f9-a124-70e0238daa06",
   "metadata": {},
   "source": [
    "#### 3.5.3 Comparison to autoregressive generation\n",
    "\n",
    "How would you compare these results to the ones you got from nanoGPT? What are some failure modes you observe? And what could be some benefits? Do you have any thoughs on how we can improve text generation with masked models, or should we just stick to autoregressive models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd05978d-7fa1-4263-8179-ec11a46842c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf37ad-3f51-443d-879f-6632cea55f5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4 Further reading\n",
    "\n",
    "Here is some further reading material should you want to dive deeper on masked modeling.\n",
    "\n",
    "Masked image generation:\n",
    "- [MaskGIT: Masked Generative Image Transformer](https://arxiv.org/abs/2202.04200)\n",
    "- [Muse: Text-To-Image Generation via Masked Generative Transformers](https://arxiv.org/abs/2301.00704)\n",
    "- [MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis](https://arxiv.org/abs/2211.09117)\n",
    "- [Randomized Autoregressive Visual Generation](https://arxiv.org/abs/2411.00776)\n",
    "- [RandAR: Decoder-only Autoregressive Visual Generation in Random Orders](https://arxiv.org/abs/2412.01827)\n",
    "- [Autoregressive Image Generation without Vector Quantization](https://arxiv.org/abs/2406.11838)\n",
    "- [4M: Massively Multimodal Masked Modeling](https://arxiv.org/abs/2312.06647)\n",
    "\n",
    "Masked text generation:\n",
    "- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)\n",
    "- [Structured Denoising Diffusion Models in Discrete State-Spaces](https://arxiv.org/abs/2107.03006)\n",
    "- [Large Language Diffusion Models](https://arxiv.org/abs/2502.09992)\n",
    "\n",
    "Masked pre-training:\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
    "- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)\n",
    "- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)\n",
    "- [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)\n",
    "- [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254)\n",
    "- [BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers](https://arxiv.org/abs/2208.06366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1dead-53d5-4b38-9ef3-52a7cbe47ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
